#!/usr/bin/env python3
"""
RnAgent å‰ç«¯åº”ç”¨ - åŸºäºSTAgent_MCPçš„ä¼˜åŒ–ç‰ˆæœ¬
ç›´æ¥è°ƒç”¨MCPå·¥å…·ï¼Œç®€åŒ–æ¶æ„
"""

import streamlit as st
import os
import sys
import asyncio
import json
import requests
import logging
from typing import List, Dict, Any
from datetime import datetime
from dotenv import load_dotenv
from langchain_core.messages import HumanMessage, AIMessage, ToolMessage, BaseMessage
from langchain_openai import ChatOpenAI
from pydantic import SecretStr
from mcp.client.session import ClientSession
from mcp.client.sse import sse_client
from uuid import uuid4

# è®¾ç½®è¯¦ç»†çš„æ—¥å¿—æ ¼å¼
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s [%(levelname)s] %(name)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('rna_streamlit_app.log', encoding='utf-8')
    ]
)
logger = logging.getLogger(__name__)

# MCPæœåŠ¡å™¨URL
MCP_SERVER_URL = "http://localhost:8000/sse"
# Agent Core HTTP API
AGENT_CORE_CHAT_URL = "http://localhost:8002/chat"

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="RnAgent - å•ç»†èƒRNAåˆ†ææ™ºèƒ½ä½“",
    page_icon="ğŸ§¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ===== å…¨å±€æ ·å¼ =====
st.markdown(
    """
    <style>
    .status-indicator {display:inline-block;width:10px;height:10px;border-radius:50%;margin-right:4px;}
    .status-online{background:#4caf50;}
    .status-offline{background:#f44336;}

    .chat-message{padding:8px 12px;border-radius:6px;margin-bottom:10px;}
    .user-message{background:#e1f5fe;}
    .assistant-message{background:#f1f8e9;}
    .tool-message{background:#fff3e0;}
    </style>
    """,
    unsafe_allow_html=True
)

# ä¸»æ ‡é¢˜
st.markdown('<h1 class="main-title">ğŸ§¬ RnAgent - å•ç»†èƒRNAåˆ†ææ™ºèƒ½ä½“</h1>', unsafe_allow_html=True)

# æ„å»º ToolMessage è¾…åŠ©å‡½æ•°
def build_tool_message(tool_name: str, result: Dict[str, Any]) -> ToolMessage:
    """æ ¹æ®MCPè¿”å›ç»“æœæ„é€ åŒ…å«éšæœºtool_call_idçš„ToolMessage"""
    return ToolMessage(
        content=result.get("content", ""),
        tool_call_id=str(uuid4()),
        name=tool_name,
        artifact=result.get("artifact", [])
    )

# MCPå·¥å…·è°ƒç”¨å‡½æ•°
def parse_mcp_result(result):
    """è§£æMCPå·¥å…·è¿”å›çš„ç»“æœ"""
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰contentå­—æ®µ
        if hasattr(result, 'content') and result.content:
            # è·å–ç¬¬ä¸€ä¸ªTextContentçš„æ–‡æœ¬å†…å®¹
            text_content = result.content[0].text
            # å°è¯•è§£æJSON
            try:
                parsed = json.loads(text_content)
                return parsed
            except json.JSONDecodeError:
                # å¦‚æœä¸æ˜¯JSONï¼Œç›´æ¥è¿”å›æ–‡æœ¬
                return {"content": text_content}
        else:
            return {"error": "No content in result"}
    except Exception as e:
        return {"error": f"è§£æç»“æœå¤±è´¥: {e}"}

def call_mcp_tool_sync(tool_name: str, arguments: Dict[str, Any]) -> Any:
    """åŒæ­¥è°ƒç”¨MCPå·¥å…· - ä½¿ç”¨æ­£ç¡®çš„SSEå®¢æˆ·ç«¯è¿æ¥æ–¹å¼"""
    try:
        logger.info(f"[MCPè°ƒç”¨] å·¥å…·: {tool_name}, å‚æ•°: {arguments}")
        result = asyncio.run(call_mcp_tool(tool_name, arguments))
        logger.info(f"[MCPè¿”å›] å·¥å…·: {tool_name}, è¿”å›ç±»å‹: {type(result)}")
        
        # è§£æMCPç»“æœ
        parsed_result = parse_mcp_result(result)
        logger.info(f"[MCPè§£æ] å·¥å…·: {tool_name}, è§£æå: {parsed_result}")
        return parsed_result
    except Exception as e:
        logger.error(f"è°ƒç”¨MCPå·¥å…·å¤±è´¥: {e}")
        return {"error": f"è¿æ¥MCPæœåŠ¡å™¨å¤±è´¥: {str(e)}"}

async def call_mcp_tool(tool_name: str, arguments: Dict[str, Any]) -> Any:
    """å¼‚æ­¥è°ƒç”¨MCPå·¥å…·"""
    async with sse_client(MCP_SERVER_URL) as (read, write):
        async with ClientSession(read, write) as session:
            await session.initialize()
            result = await session.call_tool(tool_name, arguments)
            return result

def check_mcp_server_health() -> bool:
    """æ£€æŸ¥MCPæœåŠ¡å™¨å¥åº·çŠ¶æ€"""
    try:
        result = call_mcp_tool_sync("health_check", {})
        # æ£€æŸ¥è§£æåçš„ç»“æœ
        return isinstance(result, dict) and not result.get("error") and result.get("status") == "healthy"
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return False

def get_llm_client(model_name: str = "gpt-4o"):
    """æ ¹æ®æ¨¡å‹åç§°è¿”å›ç›¸åº”çš„LLMå®¢æˆ·ç«¯"""
    if model_name.startswith("deepseek"):
        deepseek_api_key = os.environ.get("DEEPSEEK_API_KEY")
        if not deepseek_api_key:
            raise ValueError("DEEPSEEK_API_KEY environment variable not set")
        
        base_url = "https://www.chataiapi.com/v1"
        
        return ChatOpenAI(
            model=model_name,
            temperature=0,
            api_key=SecretStr(deepseek_api_key),
            base_url=base_url
        )
    else:
        openai_api_key = os.environ.get("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY environment variable not set")
        
        return ChatOpenAI(
            model=model_name,
            temperature=0
        )

def is_python_code(text: str) -> bool:
    """åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºPythonä»£ç """
    if not text or not isinstance(text, str):
        return False
    
    text = text.strip()
    python_indicators = [
        "import ", "from ", "def ", "class ", "plt.show()", "plt.figure",
        "sc.pl.", "adata = ", "pd.DataFrame", "np.", "matplotlib", "scanpy"
    ]
    
    if text.startswith("import ") or text.startswith("from "):
        return True
    
    if "plt.show()" in text or "plt.figure" in text:
        return True
    
    indicator_count = sum(1 for indicator in python_indicators if indicator in text)
    return indicator_count >= 2

def extract_artifacts_from_content(content: str) -> tuple[str, list]:
    """ä»å†…å®¹ä¸­æå–å›¾ç‰‡è·¯å¾„ä¿¡æ¯"""
    import re
    
    # æŸ¥æ‰¾[ARTIFACTS]æ ‡è®°
    pattern = r'\[ARTIFACTS\](.*?)\[/ARTIFACTS\]'
    match = re.search(pattern, content)
    
    if match:
        try:
            artifacts_json = match.group(1)
            artifacts = json.loads(artifacts_json)
            # ç§»é™¤æ ‡è®°ï¼Œè¿”å›æ¸…æ´çš„å†…å®¹
            clean_content = re.sub(pattern, '', content).strip()
            return clean_content, artifacts
        except json.JSONDecodeError:
            return content, []
    
    return content, []

def display_message(message: BaseMessage, index: int):
    """æ˜¾ç¤ºå•æ¡æ¶ˆæ¯"""
    # è·å–ç”¨æˆ·è®¾ç½®çš„å›¾ç‰‡å®½åº¦ï¼Œé»˜è®¤ä¸º700
    img_width = getattr(st.session_state, 'image_width', 700)
    
    with st.container():
        if isinstance(message, HumanMessage):
            st.markdown('<div class="chat-message user-message">', unsafe_allow_html=True)
            st.markdown("**ğŸ‘¤ ç”¨æˆ·:**")
            st.write(message.content)
            st.markdown('</div>', unsafe_allow_html=True)
            
        elif isinstance(message, AIMessage):
            st.markdown('<div class="chat-message assistant-message">', unsafe_allow_html=True)
            st.markdown("**ğŸ¤– åŠ©æ‰‹:**")
            
            # è§£æå†…å®¹ä¸­çš„å›¾ç‰‡è·¯å¾„ä¿¡æ¯
            content_str = str(message.content) if message.content else ""
            clean_content, artifacts = extract_artifacts_from_content(content_str)
            st.write(clean_content)
            
            # æ˜¾ç¤ºå›¾ç‰‡ï¼ˆå¦‚æœæœ‰ï¼‰
            if artifacts:
                st.write("**ç”Ÿæˆçš„å›¾è¡¨:**")
                for rel_path in artifacts:
                    if rel_path.endswith(".png"):
                        abs_path = os.path.join(
                            os.path.dirname(os.path.dirname(__file__)), 
                            "3_backend_mcp", 
                            rel_path
                        )
                        if os.path.exists(abs_path):
                            st.image(
                                abs_path, 
                                caption=f"ç”Ÿæˆçš„å›¾è¡¨: {os.path.basename(rel_path)}", 
                                width=img_width  # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„å®½åº¦
                            )
                        else:
                            st.error(f"å›¾ç‰‡æ–‡ä»¶æœªæ‰¾åˆ°: {rel_path}")
                            st.info(f"é¢„æœŸè·¯å¾„: {abs_path}")
            
            st.markdown('</div>', unsafe_allow_html=True)
            
        elif isinstance(message, ToolMessage):
            st.markdown('<div class="chat-message tool-message">', unsafe_allow_html=True)
            tool_name = getattr(message, 'name', 'Unknown Tool')
            st.markdown(f"**ğŸ”§ å·¥å…·: {tool_name}**")
            
            content = message.content
            
            if isinstance(content, str):
                # è§£æå†…å®¹ä¸­çš„å›¾ç‰‡è·¯å¾„ä¿¡æ¯
                clean_content, artifacts = extract_artifacts_from_content(content)
                
                if is_python_code(clean_content):
                    st.code(clean_content, language="python")
                    
                    # æ·»åŠ è¿è¡ŒæŒ‰é’®
                    run_key = f"run_{tool_name}_{index}_{hash(clean_content)}"
                    if st.button("â–¶ï¸ è¿è¡Œä»£ç ", key=run_key):
                        with st.spinner("æ­£åœ¨æ‰§è¡Œä»£ç ..."):
                            result = call_mcp_tool_sync("python_repl_tool", {"query": clean_content})
                            
                            if isinstance(result, dict):
                                st.write("**æ‰§è¡Œç»“æœ:**")
                                st.write(result.get("content", ""))
                                
                                # æ˜¾ç¤ºç”Ÿæˆçš„å›¾ç‰‡
                                exec_artifacts = result.get("artifact", [])
                                if exec_artifacts:
                                    st.write("**ç”Ÿæˆçš„å›¾è¡¨:**")
                                    for img_path in exec_artifacts:
                                        abs_path = os.path.join(
                                            os.path.dirname(os.path.dirname(__file__)), 
                                            "3_backend_mcp", 
                                            img_path
                                        )
                                        if os.path.exists(abs_path):
                                            st.image(
                                                abs_path, 
                                                caption=f"ç”Ÿæˆçš„å›¾è¡¨: {os.path.basename(img_path)}", 
                                                width=img_width  # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„å®½åº¦
                                            )
                                        else:
                                            st.error(f"å›¾ç‰‡æ–‡ä»¶æœªæ‰¾åˆ°: {img_path}")
                            else:
                                st.write("**æ‰§è¡Œç»“æœ:**")
                                st.write(str(result))
                else:
                    st.write(clean_content)
                
                # æ˜¾ç¤ºå·¥å…·è¿”å›çš„å›¾ç‰‡
                if artifacts:
                    st.write("**ç”Ÿæˆçš„å›¾è¡¨:**")
                    for rel_path in artifacts:
                        if rel_path.endswith(".png"):
                            abs_path = os.path.join(
                                os.path.dirname(os.path.dirname(__file__)), 
                                "3_backend_mcp", 
                                rel_path
                            )
                            if os.path.exists(abs_path):
                                st.image(
                                    abs_path, 
                                    caption=f"ç”Ÿæˆçš„å›¾è¡¨: {os.path.basename(rel_path)}", 
                                    width=img_width  # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„å®½åº¦
                                )
                            else:
                                st.error(f"å›¾ç‰‡æ–‡ä»¶æœªæ‰¾åˆ°: {rel_path}")
                                st.info(f"é¢„æœŸè·¯å¾„: {abs_path}")
            else:
                st.write(content)
            
            # æ˜¾ç¤ºå·¥å…·è¿”å›çš„å›¾ç‰‡ï¼ˆä¿ç•™åŸæœ‰é€»è¾‘ä½œä¸ºå¤‡ç”¨ï¼‰
            tool_artifacts = getattr(message, 'artifact', [])
            if tool_artifacts:
                st.write("**ç”Ÿæˆçš„å›¾è¡¨:**")
                for rel_path in tool_artifacts:
                    if rel_path.endswith(".png"):
                        abs_path = os.path.join(
                            os.path.dirname(os.path.dirname(__file__)), 
                            "3_backend_mcp", 
                            rel_path
                        )
                        if os.path.exists(abs_path):
                            st.image(
                                abs_path, 
                                caption=f"ç”Ÿæˆçš„å›¾è¡¨: {os.path.basename(rel_path)}", 
                                width=img_width  # ä½¿ç”¨ç”¨æˆ·è®¾ç½®çš„å®½åº¦
                            )
                        else:
                            st.error(f"å›¾ç‰‡æ–‡ä»¶æœªæ‰¾åˆ°: {rel_path}")
            
            st.markdown('</div>', unsafe_allow_html=True)

# åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
if "messages" not in st.session_state:
    st.session_state.messages = []

def get_available_models():
    """æ ¹æ®APIå¯†é’¥è‡ªåŠ¨æ£€æµ‹å¯ç”¨æ¨¡å‹"""
    openai_key = os.getenv("OPENAI_API_KEY")
    deepseek_key = os.getenv("DEEPSEEK_API_KEY")
    
    available_models = {}
    
    # æ£€æŸ¥OpenAI API
    if openai_key:
        available_models.update({
            "OpenAI GPT-4o": "gpt-4o",
            "OpenAI GPT-4 Turbo": "gpt-4-turbo"
        })
    
    # æ£€æŸ¥DeepSeek API  
    if deepseek_key:
        available_models.update({
            "DeepSeek Chat": "deepseek-chat"
        })
    
    return available_models, openai_key, deepseek_key

# å¦‚æœå‰é¢æ²¡æœ‰å®šä¹‰ call_agent_core_sync / check_agent_core_healthï¼Œåˆ™è¡¥å……å®šä¹‰
if 'call_agent_core_sync' not in globals():
    def call_agent_core_sync(message: str) -> Dict[str, Any]:
        try:
            resp = requests.post(AGENT_CORE_CHAT_URL, json={"message": message}, timeout=120)
            resp.raise_for_status()
            return resp.json()
        except Exception as e:
            logger.error(f"è°ƒç”¨ Agent Core å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}

if 'check_agent_core_health' not in globals():
    def check_agent_core_health() -> bool:
        try:
            resp = requests.get("http://localhost:8002/health", timeout=10)
            return resp.status_code == 200 and resp.json().get("status") == "healthy"
        except Exception:
            return False

# ä¾§è¾¹æ 
with st.sidebar:
    st.title("âš™ï¸ è®¾ç½®")
    
    # è·å–å¯ç”¨æ¨¡å‹
    available_models, openai_key, deepseek_key = get_available_models()
    
    # APIå¯†é’¥çŠ¶æ€
    st.subheader("ğŸ”‘ APIçŠ¶æ€")
    
    if openai_key:
        st.markdown('<span class="status-indicator status-online"></span>OpenAI API: å·²é…ç½®', unsafe_allow_html=True)
    else:
        st.markdown('<span class="status-indicator status-offline"></span>OpenAI API: æœªé…ç½®', unsafe_allow_html=True)
    
    if deepseek_key:
        st.markdown('<span class="status-indicator status-online"></span>DeepSeek API: å·²é…ç½®', unsafe_allow_html=True)
    else:
        st.markdown('<span class="status-indicator status-offline"></span>DeepSeek API: æœªé…ç½®', unsafe_allow_html=True)
    
    # æ¨¡å‹é€‰æ‹©
    st.subheader("ğŸ¤– æ¨¡å‹é€‰æ‹©")
    
    if available_models:
        # è‡ªåŠ¨é€‰æ‹©æœ€ä½³é»˜è®¤æ¨¡å‹
        model_keys = list(available_models.keys())
        
        # ä¼˜å…ˆçº§ï¼šOpenAI GPT-4o > OpenAI GPT-4 Turbo > DeepSeek Chat
        default_index = 0
        if "OpenAI GPT-4o" in model_keys:
            default_index = model_keys.index("OpenAI GPT-4o")
        elif "OpenAI GPT-4 Turbo" in model_keys:
            default_index = model_keys.index("OpenAI GPT-4 Turbo")
        elif "DeepSeek Chat" in model_keys:
            default_index = model_keys.index("DeepSeek Chat")
        
        selected_model_name = st.selectbox(
            "é€‰æ‹©æ¨¡å‹",
            options=model_keys,
            index=default_index,
            help="è‡ªåŠ¨æ ¹æ®æ‚¨çš„APIå¯†é’¥ç­›é€‰å¯ç”¨æ¨¡å‹"
        )
        selected_model = available_models[selected_model_name]
        
        # æ˜¾ç¤ºé€‰æ‹©çš„æ¨¡å‹ä¿¡æ¯
        if selected_model.startswith("gpt"):
            st.info("ğŸ”¸ ä½¿ç”¨OpenAIæ¨¡å‹ï¼Œéœ€è¦ç¨³å®šçš„ç½‘ç»œè¿æ¥")
        elif selected_model.startswith("deepseek"):
            st.info("ğŸ”¸ ä½¿ç”¨DeepSeekæ¨¡å‹ï¼Œç»æµå®æƒ çš„é€‰æ‹©")
            
    else:
        # æ²¡æœ‰å¯ç”¨çš„APIå¯†é’¥
        st.error("âŒ æœªæ£€æµ‹åˆ°å¯ç”¨çš„APIå¯†é’¥")
        st.markdown("""
        **è¯·è®¾ç½®ä»¥ä¸‹ç¯å¢ƒå˜é‡ä¹‹ä¸€ï¼š**
        
        ```bash
        # OpenAI API
        export OPENAI_API_KEY="your_key"
        
        # DeepSeek API
        export DEEPSEEK_API_KEY="your_key"
        ```
        
        è®¾ç½®åè¯·é‡å¯åº”ç”¨ã€‚
        """)
        
        # æä¾›ä¸€ä¸ªé»˜è®¤é€‰æ‹©ä»¥é¿å…é”™è¯¯
        selected_model = "gpt-4o"
        selected_model_name = "OpenAI GPT-4o (æœªé…ç½®)"
    
    # æœåŠ¡çŠ¶æ€
    st.subheader("ğŸ–¥ï¸ æœåŠ¡çŠ¶æ€")
    
    # æ£€æŸ¥MCPæœåŠ¡å™¨çŠ¶æ€
    if check_mcp_server_health():
        st.markdown('<span class="status-indicator status-online"></span>MCPæœåŠ¡å™¨: åœ¨çº¿', unsafe_allow_html=True)
        server_online = True
    else:
        st.markdown('<span class="status-indicator status-offline"></span>MCPæœåŠ¡å™¨: ç¦»çº¿', unsafe_allow_html=True)
        server_online = False
        st.warning("âš ï¸ MCPæœåŠ¡å™¨ç¦»çº¿ï¼Œè¯·ç¡®ä¿åç«¯æœåŠ¡å™¨æ­£åœ¨è¿è¡Œ")

    # æ£€æŸ¥Agent CoreçŠ¶æ€
    if check_agent_core_health():
        st.markdown('<span class="status-indicator status-online"></span>Agent Core: åœ¨çº¿', unsafe_allow_html=True)
        agent_online = True
    else:
        st.markdown('<span class="status-indicator status-offline"></span>Agent Core: ç¦»çº¿', unsafe_allow_html=True)
        agent_online = False
        st.warning("âš ï¸ Agent Coreç¦»çº¿ï¼ŒèŠå¤©åŠŸèƒ½å°†ä¸å¯ç”¨")
    
    # æ˜¾ç¤ºè®¾ç½®
    st.subheader("ğŸ–¼ï¸ æ˜¾ç¤ºè®¾ç½®")
    
    # å›¾ç‰‡æ˜¾ç¤ºå¤§å°è®¾ç½®
    image_width = st.slider(
        "å›¾è¡¨æ˜¾ç¤ºå®½åº¦ (åƒç´ )",
        min_value=400,
        max_value=1200,
        value=700,
        step=50,
        help="è°ƒæ•´ç”Ÿæˆå›¾è¡¨çš„æ˜¾ç¤ºå®½åº¦ï¼Œè®©å›¾ç‰‡å¤§å°æ›´ç¬¦åˆæ‚¨çš„å–œå¥½"
    )
    
    # å°†å›¾ç‰‡å®½åº¦ä¿å­˜åˆ°session state
    st.session_state.image_width = image_width
    
    # å¿«é€Ÿæ“ä½œ
    st.subheader("ğŸš€ å¿«é€Ÿæ“ä½œ")
    
    if not server_online:
        st.error("âŒ MCPæœåŠ¡å™¨ç¦»çº¿ï¼Œæ— æ³•ä½¿ç”¨å¿«é€Ÿæ“ä½œåŠŸèƒ½")
        st.info("ğŸ’¡ è¯·è¿è¡Œ `python run_rna_demo.py` å¯åŠ¨åç«¯æœåŠ¡å™¨")
    else:
        # MCPå·¥å…·æŒ‰é’®
        if st.button("ğŸ“Š åŠ è½½PBMC3Kæ•°æ®", use_container_width=True):
            with st.spinner("æ­£åœ¨è·å–æ•°æ®åŠ è½½ä»£ç ..."):
                result = call_mcp_tool_sync("load_pbmc3k_data", {})
                if isinstance(result, dict) and "content" in result:
                    tool_message = build_tool_message("load_pbmc3k_data", result)
                    st.session_state.messages.append(tool_message)
                    st.rerun()
        
        if st.button("ğŸ” è´¨é‡æ§åˆ¶åˆ†æ", use_container_width=True):
            with st.spinner("æ­£åœ¨è·å–è´¨é‡æ§åˆ¶ä»£ç ..."):
                result = call_mcp_tool_sync("quality_control_analysis", {})
                if isinstance(result, dict) and "content" in result:
                    tool_message = build_tool_message("quality_control_analysis", result)
                    st.session_state.messages.append(tool_message)
                    st.rerun()
        
        if st.button("âš™ï¸ æ•°æ®é¢„å¤„ç†", use_container_width=True):
            with st.spinner("æ­£åœ¨è·å–é¢„å¤„ç†ä»£ç ..."):
                result = call_mcp_tool_sync("preprocessing_analysis", {})
                if isinstance(result, dict) and "content" in result:
                    tool_message = build_tool_message("preprocessing_analysis", result)
                    st.session_state.messages.append(tool_message)
                    st.rerun()
        
        if st.button("ğŸ“ˆ é™ç»´åˆ†æ", use_container_width=True):
            with st.spinner("æ­£åœ¨è·å–é™ç»´åˆ†æä»£ç ..."):
                result = call_mcp_tool_sync("dimensionality_reduction_analysis", {})
                if isinstance(result, dict) and "content" in result:
                    tool_message = build_tool_message("dimensionality_reduction_analysis", result)
                    st.session_state.messages.append(tool_message)
                    st.rerun()
        
        if st.button("ğŸ¯ èšç±»åˆ†æ", use_container_width=True):
            with st.spinner("æ­£åœ¨è·å–èšç±»åˆ†æä»£ç ..."):
                result = call_mcp_tool_sync("clustering_analysis", {})
                if isinstance(result, dict) and "content" in result:
                    tool_message = build_tool_message("clustering_analysis", result)
                    st.session_state.messages.append(tool_message)
                    st.rerun()
        
        if st.button("ğŸ§¬ æ ‡è®°åŸºå› åˆ†æ", use_container_width=True):
            with st.spinner("æ­£åœ¨è·å–æ ‡è®°åŸºå› åˆ†æä»£ç ..."):
                result = call_mcp_tool_sync("marker_genes_analysis", {})
                if isinstance(result, dict) and "content" in result:
                    tool_message = build_tool_message("marker_genes_analysis", result)
                    st.session_state.messages.append(tool_message)
                    st.rerun()
        
        if st.button("ğŸ“‹ ç”Ÿæˆåˆ†ææŠ¥å‘Š", use_container_width=True):
            with st.spinner("æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š..."):
                result = call_mcp_tool_sync("generate_analysis_report", {})
                if isinstance(result, dict) and "content" in result:
                    tool_message = build_tool_message("generate_analysis_report", result)
                    st.session_state.messages.append(tool_message)
                    st.rerun()
        
        st.divider()
        
        if st.button("ğŸš€ å®Œæ•´åˆ†ææµç¨‹", use_container_width=True, type="primary"):
            # ä¾æ¬¡æ‰§è¡Œæ‰€æœ‰åˆ†ææ­¥éª¤
            analysis_steps = [
                ("load_pbmc3k_data", "ğŸ“Š åŠ è½½æ•°æ®"),
                ("quality_control_analysis", "ğŸ” è´¨é‡æ§åˆ¶"),
                ("preprocessing_analysis", "âš™ï¸ æ•°æ®é¢„å¤„ç†"),
                ("dimensionality_reduction_analysis", "ğŸ“ˆ é™ç»´åˆ†æ"),
                ("clustering_analysis", "ğŸ¯ èšç±»åˆ†æ"),
                ("marker_genes_analysis", "ğŸ§¬ æ ‡è®°åŸºå› åˆ†æ"),
                ("generate_analysis_report", "ğŸ“‹ ç”ŸæˆæŠ¥å‘Š")
            ]
            
            with st.spinner("æ­£åœ¨æ‰§è¡Œå®Œæ•´åˆ†ææµç¨‹..."):
                for tool_name, description in analysis_steps:
                    st.info(f"æ­£åœ¨æ‰§è¡Œ: {description}")
                    result = call_mcp_tool_sync(tool_name, {})
                    if isinstance(result, dict) and "content" in result:
                        tool_message = build_tool_message(tool_name, result)
                        st.session_state.messages.append(tool_message)
                    else:
                        st.error(f"æ‰§è¡Œ {description} æ—¶å‡ºé”™")
                        break
                
                st.success("âœ… å®Œæ•´åˆ†ææµç¨‹æ‰§è¡Œå®Œæ¯•ï¼")
                st.rerun()
    
    if st.button("ğŸ”„ æ¸…ç©ºå¯¹è¯", use_container_width=True):
        st.session_state.messages = []
        st.rerun()
    
    # æ•°æ®é›†ä¿¡æ¯
    st.subheader("ğŸ“ æ•°æ®é›†ä¿¡æ¯")
    st.info("""
    **PBMC3Kæ•°æ®é›†**
    - ç»†èƒç±»å‹: å¤–å‘¨è¡€å•æ ¸ç»†èƒ
    - å¹³å°: 10X Genomics
    - è·¯å¾„: `/Volumes/T7/å“ˆå°”æ»¨å·¥ä¸šå¤§å­¦-2025/è¯¾é¢˜ç»„é¡¹ç›®/Agent-é¡¹ç›®/PBMC3kRNA-seq/filtered_gene_bc_matrices/hg19/`
    """)

# ä¸»è¦å†…å®¹åŒºåŸŸ
chat_container = st.container()

with chat_container:
    # æ˜¾ç¤ºæ¬¢è¿ä¿¡æ¯
    if not st.session_state.messages:
        # è·å–å½“å‰å¯ç”¨æ¨¡å‹ä¿¡æ¯ç”¨äºæ¬¢è¿ä¿¡æ¯
        current_models, current_openai, current_deepseek = get_available_models()
        
        welcome_msg = """
        ### ğŸ‘‹ æ¬¢è¿ä½¿ç”¨RnAgentï¼
        
        æˆ‘æ˜¯æ‚¨çš„å•ç»†èƒRNAåˆ†æåŠ©æ‰‹ã€‚æ‚¨å¯ä»¥ï¼š
        
        1. **ä½¿ç”¨ä¾§è¾¹æ å¿«é€Ÿæ“ä½œ**ï¼šç‚¹å‡»æŒ‰é’®æ‰§è¡Œé¢„å®šä¹‰çš„åˆ†ææ­¥éª¤
        2. **åœ¨ä¸‹æ–¹è¾“å…¥è‡ªç„¶è¯­è¨€é—®é¢˜**ï¼šæˆ‘ä¼šä¸ºæ‚¨ç”Ÿæˆç›¸åº”çš„åˆ†æä»£ç   
        3. **è¿è¡Œä»£ç å¹¶æŸ¥çœ‹ç»“æœ**ï¼šç”Ÿæˆçš„å›¾è¡¨ä¼šè‡ªåŠ¨æ˜¾ç¤º
        
        **æ¨èå¼€å§‹æ–¹å¼**ï¼š
        - ç‚¹å‡»ä¾§è¾¹æ çš„"ğŸš€ å®Œæ•´åˆ†ææµç¨‹"æŒ‰é’®è¿›è¡Œç«¯åˆ°ç«¯åˆ†æ
        - æˆ–è€…é€æ­¥ç‚¹å‡»å„ä¸ªåˆ†ææ­¥éª¤
        - ä¹Ÿå¯ä»¥ç›´æ¥åœ¨ä¸‹æ–¹æé—®ï¼Œå¦‚"è¯·åŠ è½½PBMC3Kæ•°æ®å¹¶æ˜¾ç¤ºåŸºæœ¬ä¿¡æ¯"
        
        """
        
        # æ ¹æ®APIå¯†é’¥çŠ¶æ€æ·»åŠ ç›¸åº”ä¿¡æ¯
        if current_models:
            available_model_names = list(current_models.keys())
            welcome_msg += f"""
        **ğŸ¤– AIæ¨¡å‹çŠ¶æ€**ï¼š
        - âœ… å·²è‡ªåŠ¨æ£€æµ‹å¹¶é…ç½®å¯ç”¨æ¨¡å‹ï¼š{', '.join(available_model_names)}
        - ğŸ¯ å½“å‰é€‰æ‹©ï¼šå·²è‡ªåŠ¨ä¸ºæ‚¨é€‰æ‹©æœ€ä½³æ¨¡å‹
        """
        else:
            welcome_msg += """
        **âš ï¸ AIæ¨¡å‹çŠ¶æ€**ï¼š
        - âŒ æœªæ£€æµ‹åˆ°APIå¯†é’¥ï¼Œè‡ªç„¶è¯­è¨€å¯¹è¯åŠŸèƒ½ä¸å¯ç”¨
        - ğŸ’¡ æ‚¨ä»å¯ä½¿ç”¨æ‰€æœ‰å¿«é€Ÿæ“ä½œæŒ‰é’®è¿›è¡Œåˆ†æ
        - ğŸ”§ è¯·åœ¨ä¾§è¾¹æ æŸ¥çœ‹APIé…ç½®è¯´æ˜
        """
        
        st.markdown(welcome_msg)
    
    # æ˜¾ç¤ºå¯¹è¯å†å²
    for i, message in enumerate(st.session_state.messages):
        display_message(message, i)

# èŠå¤©è¾“å…¥
if prompt := st.chat_input("è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæ¯”å¦‚'è¯·åˆ†æPBMC3Kæ•°æ®çš„èšç±»ç»“æœ'..."):
    # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append(HumanMessage(content=prompt))
    
    # é‡æ–°è·å–å¯ç”¨æ¨¡å‹ä¿¡æ¯ï¼ˆç¡®ä¿åœ¨èŠå¤©è¾“å…¥éƒ¨åˆ†ä¹Ÿèƒ½è®¿é—®ï¼‰
    available_models, _, _ = get_available_models()
    
    # ç®€å•çš„æ„å›¾è¯†åˆ«å’ŒMCPå·¥å…·è°ƒç”¨
    prompt_lower = prompt.lower()
    
    if "åŠ è½½" in prompt and ("æ•°æ®" in prompt or "pbmc" in prompt_lower):
        with st.spinner("æ­£åœ¨è·å–æ•°æ®åŠ è½½ä»£ç ..."):
            result = call_mcp_tool_sync("load_pbmc3k_data", {})
            if isinstance(result, dict) and "content" in result:
                tool_message = build_tool_message("load_pbmc3k_data", result)
                st.session_state.messages.append(tool_message)
    
    elif "è´¨é‡æ§åˆ¶" in prompt or "è´¨æ§" in prompt:
        with st.spinner("æ­£åœ¨è·å–è´¨é‡æ§åˆ¶åˆ†æä»£ç ..."):
            result = call_mcp_tool_sync("quality_control_analysis", {})
            if isinstance(result, dict) and "content" in result:
                tool_message = build_tool_message("quality_control_analysis", result)
                st.session_state.messages.append(tool_message)
    
    elif "é¢„å¤„ç†" in prompt:
        with st.spinner("æ­£åœ¨è·å–æ•°æ®é¢„å¤„ç†ä»£ç ..."):
            result = call_mcp_tool_sync("preprocessing_analysis", {})
            if isinstance(result, dict) and "content" in result:
                tool_message = build_tool_message("preprocessing_analysis", result)
                st.session_state.messages.append(tool_message)
    
    elif "é™ç»´" in prompt or "pca" in prompt_lower or "umap" in prompt_lower:
        with st.spinner("æ­£åœ¨è·å–é™ç»´åˆ†æä»£ç ..."):
            result = call_mcp_tool_sync("dimensionality_reduction_analysis", {})
            if isinstance(result, dict) and "content" in result:
                tool_message = build_tool_message("dimensionality_reduction_analysis", result)
                st.session_state.messages.append(tool_message)
    
    elif "èšç±»" in prompt or "clustering" in prompt_lower:
        with st.spinner("æ­£åœ¨è·å–èšç±»åˆ†æä»£ç ..."):
            result = call_mcp_tool_sync("clustering_analysis", {})
            if isinstance(result, dict) and "content" in result:
                tool_message = build_tool_message("clustering_analysis", result)
                st.session_state.messages.append(tool_message)
    
    elif "æ ‡è®°åŸºå› " in prompt or "marker" in prompt_lower:
        with st.spinner("æ­£åœ¨è·å–æ ‡è®°åŸºå› åˆ†æä»£ç ..."):
            result = call_mcp_tool_sync("marker_genes_analysis", {})
            if isinstance(result, dict) and "content" in result:
                tool_message = build_tool_message("marker_genes_analysis", result)
                st.session_state.messages.append(tool_message)
    
    elif "æŠ¥å‘Š" in prompt or "æ€»ç»“" in prompt:
        with st.spinner("æ­£åœ¨ç”Ÿæˆåˆ†ææŠ¥å‘Š..."):
            result = call_mcp_tool_sync("generate_analysis_report", {})
            if isinstance(result, dict) and "content" in result:
                tool_message = build_tool_message("generate_analysis_report", result)
                st.session_state.messages.append(tool_message)
    
    else:
        # å¯¹äºå…¶ä»–é—®é¢˜ï¼Œè½¬äº¤Agent Coreå¤„ç†
        if agent_online:
            with st.spinner("Agent Coreå¤„ç†ä¸­..."):
                result = call_agent_core_sync(prompt)
                if result.get("success"):
                    ai_message = AIMessage(content=result.get("final_response", ""))
                    st.session_state.messages.append(ai_message)
                else:
                    error_msg = result.get("error", "æœªçŸ¥é”™è¯¯")
                    st.session_state.messages.append(AIMessage(content=f"Agent Coreé”™è¯¯: {error_msg}"))
        else:
            st.session_state.messages.append(AIMessage(content="âš ï¸ Agent Coreæœªå¯åŠ¨ï¼Œæ— æ³•å¤„ç†è¯¥è¯·æ±‚ã€‚"))
    
    st.rerun() 